## Subject: [[Data Structures]]
## Tags:
#dataStructures #computerScience #spring23 
## Date: 2023-01-31
## Lecture: 7.0 Big O Notation and Basic Recursion

## Announcements
- Exam 1 is Thursday

## 7.1 Algorithm Analysis
> [!important] We want to do better than just implementing and testing every idea we have. We want to know why one algorithm is better than another. We want to know the best we can do.
> > [!question] How do we do it?
> > > [!todo] There are several options:
> > > - Don't do analysis; just use the first algorithm you can think of that works.
> > > - Implement and time algorithms to choose the best.
> > > - Analyze algorithms by counting operations while assigning different weights to different types of operations based on how long each takes.
> > > - Analyze algorithms by assuming each operation requires the same amount of time. Count the total number of operations, and then multiply this count by the average cost of an operation.

## 7.2 Exercise: Counting Example
- Suppose `arr` is an array of `n` doubles. Here is a simple fragment of code to sum of the values in the array.
```c++
double sum = 0;
for (int i=0; i<n; ++i){
	sum += arr[i]
}
```
> [!question] What is the total number of operations performed in executing this fragment?
> > [!todo] Two

## 7.3 Exercise: Which Algorithm is Best?
- A venture capitalist is trying to decide which of 3 startup companies to invest in and has asked for your help. Hereâ€™s the timing data for their prototype software on some different size test cases:
![[7.0 Big O Notation and Basic Recursion.png | 400]]
> [!question] Which company has the best algorithm?
> > [!todo] A is the best because the at a higher n value the time is much shorter than the rest of them. It scales much better.

## 7.4 Order Notation Definition
- **Definition**: Algorithm A is order f (n) â€” denoted O(f(n)) â€” if constants k and n0 exist such that A requires no more than k âˆ— f (n) time units (operations) to solve a problem of size n â‰¥ n0.
- For example, algorithms requiring 3n + 2, 5n -3, and 14 + 17n operations are all O(n).
	- This is because we can select values for k and n0 such that the definition above holds.
	- Likewise, algorithms requiring n^2/10 + 15n - 3 and 10000 +35n^2 are all O(n^2).
- We determine the order by finding the *asymptotically dominant term (function of n)* and throwing out the leading constant.
	- We don't need to quibble about small differences in the numbers of operations.
	- We also do not need to worry about the different costs of different types of operations.
	- We don't produce and actual time. We just obtain a rough count of the number of operation.
- In practice, this makes analysis relatively simple, quick and (sometimes) rough.

## 7.5 Common Orders of Magnitude
- O(1), a.k.a *CONSTANT*: The number of operations is independent of the size of the problem.
- O(log n), aka *LOGARITHMIC*: Dictionary lookup, binary search.
- O(n), aka *LINEAR*: Sum up a list
- O(n log n): Sorting
- O(n^2), O(n^3), O(n^k), aka *POLYNOMIAL*: Find closest pair of points.
- O(2^n), O(k^n), aka *EXPONENTIAL*: Fibonacci, playing chess

## 7.6 Exercise: A Slightly Harder Example
- 

## Tasks:
- [x] Exam 1 #dataStructures ðŸ“… 2023-02-02 âœ… 2023-02-03